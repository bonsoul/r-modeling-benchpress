---
title: "xgboost for childcare costs"
author: "Bonsoul"
date: "`r Sys.Date()`"
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set( cache = TRUE, cache.lazy = FALSE, warning = FALSE, message = FALSE ,  echo = TRUE , dpi = 180, fig.width = 8, fig.height = 5)
```

```{r, set up}

library(tidytuesdayR)
library(silgelib)
library(ggplot2)
theme_set(theme_plex())
update_geom_defaults("rect", list(fill = "midnightblue", alpha = 0.8))
update_geom_defaults("line", list(color = "midnightblue", alpha = 0.8))


```

```{r}

library(tidyverse)

tuesdata <- tidytuesdayR::tt_load('2023-05-09')
tuesdata <- tidytuesdayR::tt_load(2023, week = 19)

childcare_costs <- tuesdata$childcare_costs
counties <- tuesdata$counties

```




```{r}
childcare_costs |>
  ggplot(aes(study_year ,mcsa, group = study_year, fill = study_year)) +
  geom_boxplot(alpha = 0.8, show.legend = FALSE) +
  scale_fill_distiller(palette = "RdPu")


```

```{r}

childcare_costs |>
  ggplot(aes(mhi_2018, mcsa, color = flfpr_20to64)) +
  geom_point( alpha = 0.5) +
  scale_x_log10() +
  scale_color_viridis_c()
  
```


```{r, race}

childcare_costs |> 
  select(mcsa, mhi_2018, starts_with("one_race")) |>
  select(-one_race) |>
  pivot_longer(starts_with("one_race")) |>
  ggplot(aes(value, mcsa, color = mhi_2018)) +
  geom_point(alpha = 0.4) +
  facet_wrap(vars(name), scales = "free_x") +
  scale_color_viridis_c()

```

##model

```{r}

library(tidymodels)

childcare_split <- 
  childcare_costs |>
  select(-matches("^mc_|^mf")) |>
  select(-county_fips_code) |>
  na.omit() |>
  initial_split(strata  =mcsa)

childcare_train <- training(childcare_split)
childcare_test <- testing(childcare_split)

set.seed(234)
childcare_set <- validation_split(childcare_train)
childcare_set

```

```{r}
library(recipes)

xgb_rec <- recipe(mcsa ~ ., data = childcare_train)

```


```{r}
prep(xgb_rec, training = childcare_train) %>%
  juice()

```


```{r}
library(parsnip)

xgb_spec <- boost_tree(
  trees = tune(),
  tree_depth  = tune(),
  learn_rate = tune(),
  loss_reduction = tune(),
  sample_size = tune(),
  mtry = tune()
) %>%
  set_engine("xgboost") %>%
  set_mode("regression")

```


```{r}
xgb_spec <- 
  boost_tree(
    trees = 500,
    min_n = time(),
    mtry = tune(),
    stop_iter = tune(),
    learn_rate = 0.01
  ) |>
  set_engine("xgboost", validation = 0.2) |>
  set_mode("regression")


xgb_wf <- workflow(mcsa ~ ., xgb_spec)

```


```{r}
class(recipe)

```

```{r}
colnames(training_data)

```



```{r}
library(recipes)

xgb_rec <- recipe(outcome ~ ., data = training_data) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_zv(all_predictors())

```


```{r}
prep(recipe, training = training_data) %>%
  juice()

```


```{r}
doParallel::registerDoParallel()
set.seed(234)
xgb_rs <- tune_grid(xgb_wf, childcare_set, grid = 15)
xgb_rs
```


```{r}

```

